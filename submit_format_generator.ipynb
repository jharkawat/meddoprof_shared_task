{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_dataset):\n",
    "    \"\"\"Load dataset into memory from text file\"\"\"\n",
    "    dataset = []\n",
    "    with open(path_dataset) as f:\n",
    "        words, tags = [], []\n",
    "        # Each line of the file corresponds to one word and tag\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                line = line.strip('\\n')\n",
    "                if len(line.split()) > 0:\n",
    "                    word = line.split()[0]\n",
    "                    #tag = line.split()[-1]\n",
    "                else:\n",
    "                    word = line.split()[0]\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                try:\n",
    "                    if len(word) > 0:\n",
    "                        word = str(word)\n",
    "                        words.append(word)\n",
    "                        #tags.append(tag)\n",
    "                except Exception as e:\n",
    "                    print('An exception was raised, skipping a word: {}'.format(e))\n",
    "            else:\n",
    "                if len(words) > 0:\n",
    "                    #assert len(words) == len(tags)\n",
    "                    dataset.append((words))\n",
    "                    words, tags = [], []\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, save_dir,file_name):\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    print('Saving in {}...'.format(save_dir))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Export the dataset\n",
    "    with open(os.path.join(save_dir, file_name), 'w') as file_sentences:\n",
    "        for sent in dataset:\n",
    "            for word in sent:\n",
    "                file_sentences.writelines('%s\\n' %word)\n",
    "            file_sentences.writelines('\\n')\n",
    "            #file_tags.write('{}\\n'.format(' '.join(tags)))\n",
    "        file_sentences.writelines('\\n')\n",
    "        file_sentences.writelines('\\n')\n",
    "    print('- done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'submission_Conll'\n",
    "import glob\n",
    "filenames = glob.glob('output_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_files = []\n",
    "for filename in filenames:\n",
    "    pred_tags_file = load_dataset(filename)\n",
    "    pred_tags_files.append(pred_tags_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = \"Realizamos en la consulta de Medicina de Familia ecografía cervical clínica observándose adenopatías laterocervicales y submandibulares, fundamentalmente hipoecogénicas con tendencia a la agrupación, afectando de forma bilateral a las cadenas yugulocarotídeas, en general subcentimétricas y con hilio graso conservado, sin necrosis. La mayor mide 2,5 cm de longitud por 1,1 cm de diámetro antero-posterior. En la ecografía abdominal clínica se observa hígado y bazo con tamaño en el límite alto de la normalidad, pero considerando la altura y talla de la paciente podrían considerarse aumentados de tamaño. En ecocardioscopia clínica: Ventrículo izquierdo (VI) hiperdinámico, sin identificarse alteraciones valvulares groseras. Tamaño y función del VI y ventrículo derecho normales. Ausencia de derrame pericárdico.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(space_remover(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "root_dir = 'data/meddo/test_data/Conll_Format/'\n",
    "for filename in filenames:\n",
    "    pred_tags_file = load_dataset(filename)\n",
    "    re_name = filename.split('/')[-1]\n",
    "    words_file = load_dataset(root_dir + re_name )\n",
    "    for i in range(len(pred_tags_file)):\n",
    "        for j in range(len(pred_tags_file[i])):\n",
    "            words_file[i][j] = words_file[i][j]+\"\\t\"+pred_tags_file[i][j]\n",
    "    save_dataset(words_file,save_dir,re_name)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_data/casos_clinicos_profesiones230.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file = glob.glob(\"./data/meddo/interactive/sentences/*.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conll(sentences_file, filenames):\n",
    "    with open(sentences_file) as f, open(filenames) as q:\n",
    "        assert sentences_file.split(\"/\")[-1] == filenames.split(\"/\")[-1]\n",
    "        words, tags, comb_str = [], [], []\n",
    "        for line_sentence, line_tags in zip(f,q):\n",
    "            try:\n",
    "                assert len(space_remover(line_sentence)) == len(space_remover(line_tags))\n",
    "                words.append(space_remover(line_sentence))\n",
    "                tags.append(space_remover(line_tags))\n",
    "            except:\n",
    "                token_list = space_remover(line_sentence)\n",
    "                tag_list = space_remover(line_tags)\n",
    "                \n",
    "                if len(token_list) <= 0 and len(tag_list) <= 0:\n",
    "                    continue \n",
    "                    \n",
    "                if token_list[-1] == '\\n':\n",
    "                    token_list = token_list[:-1]\n",
    "                if tag_list[-1] == '\\n':\n",
    "                    tag_list = tag_list[:-1]\n",
    "                \n",
    "                for i in range(len(token_list) - len(tag_list)):\n",
    "                    tag_list.append('O')\n",
    "                \n",
    "                print(len(tag_list), len(token_list))\n",
    "                assert len(tag_list) == len(token_list)\n",
    "                words.append(token_list)\n",
    "                tags.append(tag_list)\n",
    "                #print(\"the following file name is wrong\"+ str(sentences_file.split(\"/\")[-1]))\n",
    "        for tokens,tag in zip(words,tags):\n",
    "            for token,t in zip(tokens, tag):\n",
    "                if token == '\\n' and t == '\\n':\n",
    "                    comb_str.append(\"\\n\")\n",
    "                else:\n",
    "                    comb_str.append(token+ \"\\t\" + t)\n",
    "        #print(comb_str)\n",
    "        #exit()\n",
    "        with open(\"./temp/\"+sentences_file.split(\"/\")[-1], 'w') as r:\n",
    "            for item in comb_str:\n",
    "                if item == '\\n':\n",
    "                    r.write(\"%s\" % item)\n",
    "                else:\n",
    "                    r.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_remover(s):\n",
    "    out = re.findall(r'\\S+|\\n',s)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_conll(sentences_file[1], filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_conll(sentences_file[130], filenames[130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 189\n",
      "92 92\n",
      "77 77\n",
      "24 24\n",
      "72 72\n",
      "76 76\n",
      "227 227\n",
      "165 165\n",
      "40 40\n",
      "40 40\n",
      "183 183\n",
      "77 77\n",
      "104 104\n",
      "63 63\n",
      "17 17\n",
      "130 130\n",
      "43 43\n",
      "50 50\n",
      "117 117\n",
      "103 103\n",
      "15 15\n",
      "43 43\n",
      "4 4\n",
      "81 81\n",
      "5 5\n",
      "53 53\n",
      "45 45\n",
      "7 7\n",
      "360 360\n",
      "19 19\n",
      "4 4\n",
      "26 26\n",
      "186 186\n",
      "174 174\n",
      "177 177\n",
      "52 52\n",
      "8 8\n",
      "11 11\n",
      "11 11\n",
      "133 133\n",
      "37 37\n",
      "43 43\n",
      "130 130\n",
      "75 75\n",
      "66 66\n",
      "89 89\n",
      "92 92\n",
      "44 44\n",
      "34 34\n",
      "6 6\n",
      "36 36\n",
      "177 177\n",
      "33 33\n",
      "208 208\n",
      "58 58\n",
      "33 33\n",
      "201 201\n",
      "455 455\n",
      "412 412\n",
      "145 145\n",
      "103 103\n",
      "15 15\n",
      "196 196\n",
      "12 12\n",
      "49 49\n",
      "9 9\n",
      "27 27\n",
      "154 154\n",
      "555 555\n",
      "37 37\n",
      "44 44\n",
      "27 27\n",
      "62 62\n",
      "88 88\n",
      "145 145\n",
      "196 196\n",
      "407 407\n",
      "146 146\n",
      "30 30\n",
      "15 15\n",
      "36 36\n",
      "114 114\n",
      "30 30\n",
      "87 87\n",
      "65 65\n",
      "435 435\n",
      "109 109\n",
      "26 26\n",
      "81 81\n",
      "32 32\n",
      "2 2\n",
      "160 160\n",
      "126 126\n",
      "207 207\n",
      "64 64\n",
      "70 70\n",
      "122 122\n",
      "105 105\n",
      "11 11\n",
      "99 99\n",
      "184 184\n",
      "195 195\n",
      "51 51\n",
      "36 36\n",
      "69 69\n",
      "380 380\n",
      "19 19\n",
      "143 143\n",
      "69 69\n",
      "298 298\n",
      "26 26\n",
      "27 27\n",
      "50 50\n",
      "916 916\n",
      "386 386\n",
      "21 21\n",
      "790 790\n",
      "14 14\n",
      "26 26\n",
      "133 133\n",
      "11 11\n",
      "16 16\n",
      "502 502\n",
      "191 191\n",
      "91 91\n",
      "222 222\n",
      "479 479\n",
      "699 699\n",
      "175 175\n",
      "214 214\n",
      "119 119\n",
      "36 36\n",
      "112 112\n",
      "68 68\n",
      "632 632\n",
      "53 53\n",
      "200 200\n",
      "25 25\n",
      "342 342\n",
      "135 135\n",
      "63 63\n",
      "82 82\n",
      "53 53\n",
      "70 70\n",
      "111 111\n",
      "33 33\n",
      "133 133\n",
      "209 209\n",
      "148 148\n",
      "3 3\n",
      "350 350\n",
      "7 7\n",
      "53 53\n",
      "137 137\n",
      "508 508\n",
      "58 58\n",
      "67 67\n",
      "186 186\n",
      "101 101\n",
      "173 173\n",
      "2 2\n",
      "135 135\n",
      "67 67\n",
      "30 30\n",
      "214 214\n",
      "72 72\n",
      "309 309\n",
      "8 8\n",
      "165 165\n",
      "183 183\n",
      "389 389\n",
      "11 11\n",
      "30 30\n",
      "572 572\n",
      "12 12\n",
      "53 53\n",
      "60 60\n",
      "18 18\n",
      "419 419\n",
      "328 328\n",
      "206 206\n",
      "11 11\n",
      "13 13\n",
      "576 576\n",
      "83 83\n",
      "29 29\n",
      "211 211\n",
      "299 299\n",
      "162 162\n",
      "32 32\n",
      "480 480\n",
      "32 32\n",
      "190 190\n",
      "169 169\n",
      "45 45\n",
      "102 102\n",
      "116 116\n",
      "5 5\n",
      "219 219\n",
      "134 134\n",
      "36 36\n",
      "42 42\n",
      "346 346\n",
      "41 41\n",
      "142 142\n",
      "76 76\n",
      "19 19\n",
      "43 43\n",
      "281 281\n",
      "17 17\n",
      "18 18\n",
      "465 465\n",
      "7 7\n",
      "33 33\n",
      "26 26\n",
      "18 18\n",
      "46 46\n",
      "17 17\n",
      "51 51\n",
      "54 54\n",
      "102 102\n",
      "2 2\n",
      "17 17\n",
      "3 3\n",
      "33 33\n",
      "251 251\n",
      "36 36\n",
      "103 103\n",
      "330 330\n",
      "238 238\n",
      "55 55\n",
      "89 89\n",
      "49 49\n",
      "37 37\n",
      "8 8\n",
      "216 216\n",
      "34 34\n",
      "15 15\n",
      "258 258\n",
      "14 14\n",
      "77 77\n",
      "58 58\n",
      "434 434\n",
      "131 131\n",
      "190 190\n",
      "28 28\n",
      "39 39\n",
      "61 61\n",
      "51 51\n",
      "18 18\n",
      "113 113\n",
      "136 136\n",
      "62 62\n",
      "18 18\n",
      "133 133\n",
      "14 14\n",
      "77 77\n",
      "15 15\n",
      "38 38\n",
      "41 41\n",
      "681 681\n",
      "358 358\n",
      "77 77\n",
      "227 227\n",
      "12 12\n",
      "45 45\n",
      "89 89\n",
      "499 499\n",
      "47 47\n",
      "57 57\n",
      "68 68\n",
      "63 63\n",
      "75 75\n",
      "21 21\n",
      "200 200\n",
      "40 40\n",
      "69 69\n",
      "7 7\n",
      "566 566\n",
      "62 62\n",
      "448 448\n",
      "121 121\n",
      "128 128\n",
      "4 4\n",
      "42 42\n",
      "6 6\n",
      "172 172\n",
      "18 18\n",
      "4 4\n",
      "5 5\n",
      "597 597\n",
      "563 563\n",
      "107 107\n",
      "306 306\n",
      "16 16\n",
      "88 88\n",
      "643 643\n",
      "87 87\n",
      "11 11\n",
      "84 84\n",
      "30 30\n",
      "453 453\n",
      "43 43\n",
      "36 36\n",
      "127 127\n",
      "79 79\n",
      "31 31\n",
      "50 50\n",
      "182 182\n",
      "4 4\n",
      "27 27\n",
      "36 36\n",
      "47 47\n",
      "20 20\n",
      "6 6\n",
      "31 31\n",
      "14 14\n",
      "65 65\n",
      "555 555\n",
      "96 96\n",
      "284 284\n",
      "6 6\n",
      "58 58\n",
      "261 261\n",
      "168 168\n",
      "248 248\n",
      "33 33\n",
      "58 58\n",
      "36 36\n",
      "54 54\n",
      "80 80\n",
      "260 260\n",
      "756 756\n",
      "163 163\n",
      "257 257\n",
      "50 50\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for sentences_f, file_n in zip(sentences_file,filenames):\n",
    "    generate_conll(sentences_f, file_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_class = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(bert_class, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
