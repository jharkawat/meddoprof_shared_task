{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_dataset):\n",
    "    \"\"\"Load dataset into memory from text file\"\"\"\n",
    "    dataset = []\n",
    "    with open(path_dataset) as f:\n",
    "        words, tags = [], []\n",
    "        # Each line of the file corresponds to one word and tag\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                line = line.strip('\\n')\n",
    "                if len(line.split()) > 0:\n",
    "                    word = line.split()[0]\n",
    "                    #tag = line.split()[-1]\n",
    "                else:\n",
    "                    word = line.split()[0]\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                try:\n",
    "                    if len(word) > 0:\n",
    "                        word = str(word)\n",
    "                        words.append(word)\n",
    "                        #tags.append(tag)\n",
    "                except Exception as e:\n",
    "                    print('An exception was raised, skipping a word: {}'.format(e))\n",
    "            else:\n",
    "                if len(words) > 0:\n",
    "                    #assert len(words) == len(tags)\n",
    "                    dataset.append((words))\n",
    "                    words, tags = [], []\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, save_dir,file_name):\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    print('Saving in {}...'.format(save_dir))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Export the dataset\n",
    "    with open(os.path.join(save_dir, file_name), 'w') as file_sentences:\n",
    "        for sent in dataset:\n",
    "            for word in sent:\n",
    "                file_sentences.writelines('%s\\n' %word)\n",
    "            file_sentences.writelines('\\n')\n",
    "            #file_tags.write('{}\\n'.format(' '.join(tags)))\n",
    "        file_sentences.writelines('\\n')\n",
    "        file_sentences.writelines('\\n')\n",
    "    print('- done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'submission_Conll'\n",
    "import glob\n",
    "filenames = glob.glob('output_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_files = []\n",
    "for filename in filenames:\n",
    "    pred_tags_file = load_dataset(filename)\n",
    "    pred_tags_files.append(pred_tags_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/meddo/test_data/Conll_Format/casos_clinicos_profesiones230.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-df1801163f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpred_tags_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mre_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwords_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mre_name\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-97ca474ef1bf>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path_dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Load dataset into memory from text file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Each line of the file corresponds to one word and tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/meddo/test_data/Conll_Format/casos_clinicos_profesiones230.txt'"
     ]
    }
   ],
   "source": [
    "root_dir = 'data/meddo/test_data/Conll_Format/'\n",
    "for filename in filenames:\n",
    "    pred_tags_file = load_dataset(filename)\n",
    "    re_name = filename.split('/')[-1]\n",
    "    words_file = load_dataset(root_dir + re_name )\n",
    "    for i in range(len(pred_tags_file)):\n",
    "        for j in range(len(pred_tags_file[i])):\n",
    "            words_file[i][j] = words_file[i][j]+\"\\t\"+pred_tags_file[i][j]\n",
    "    save_dataset(words_file,save_dir,re_name)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_data/casos_clinicos_profesiones230.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file = glob.glob(\"./data/meddo/interactive/sentences/*.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conll(sentences_file, filenames):\n",
    "    with open(sentences_file) as f, open(filenames) as q:\n",
    "        assert sentences_file.split(\"/\")[-1] == filenames.split(\"/\")[-1]\n",
    "        words, tags, comb_str = [], [], []\n",
    "        for line_sentence, line_tags in zip(f,q):\n",
    "            try:\n",
    "                assert len(line_sentence.split()) == len(line_tags.split('    '))\n",
    "                words.append(line_sentence.split())\n",
    "                tags.append(line_tags.split('    '))\n",
    "            except:\n",
    "                token_list = line_sentence.split()\n",
    "                tag_list = line_tags.split('    ')\n",
    "                if len(token_list) <= 0:\n",
    "                    continue\n",
    "                for i in range(len(token_list) - len(tag_list)):\n",
    "                    tag_list.append('O')\n",
    "                print(len(tag_list), len(token_list))\n",
    "                assert len(tag_list) == len(token_list)\n",
    "                words.append(token_list)\n",
    "                tags.append(tag_list)\n",
    "                #print(\"the following file name is wrong\"+ str(sentences_file.split(\"/\")[-1]))\n",
    "        for tokens,tag in zip(words,tags):\n",
    "            for token,t in zip(tokens, tag):\n",
    "                comb_str.append(token+ \"\\t\" + t)\n",
    "\n",
    "        with open(\"./temp/\"+sentences_file.split(\"/\")[-1], 'w') as r:\n",
    "            for item in comb_str:\n",
    "                r.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/meddo/interactive/sentences/caso_clinico_endocrinologia47.txt'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_file[130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Varón', 'de', '79', 'años', 'que', 'ingresa', 'en', 'el', 'Servicio', 'de', 'Reumatología', 'para', 'estudio', 'de', 'dolor', 'lumbar', 'tipo', 'mecánico', 'junto', 'a', 'elevación', 'de', 'la', 'fosfatasa', 'alcalina', 'en', 'mayo', '2014.Entre', 'sus', 'antecedentes', 'personales', 'destaca:', 'hipertensión', 'arterial,', 'diabetes', 'mellitus', 'tipo', '2', '(un', 'año', 'de', 'evolución,', 'tratamiento', 'dietético),', 'ulcus', 'péptico', 'y', 'fractura', 'de', 'clavícula', 'izquierda', 'no', 'traumática', 'en', '2013.', 'No', 'tiene', 'antecedentes', 'familiares', 'de', 'interés.', 'En', 'tratamiento', 'con', 'enalapril', '10mg.', 'No', 'tóxicos.', 'En', 'la', 'anamnesis', 'dirigida', 'presentaba', 'dolor', 'en', 'región', 'lumbar,', 'de', 'tipo', 'mecánico,', 'desde', 'hacía', 'unos', 'cuatro', 'años', 'con', 'incremento', 'progresivo', 'de', 'la', 'intensidad', 'y', 'desde', 'el', 'último', 'año', 'se', 'irradiaba', 'a', 'miembros', 'inferiores', 'con', 'dificultad', 'para', 'la', 'marcha.', 'Hacía', 'un', 'año', 'había', 'tenido', 'una', 'fractura', 'no', 'traumática', 'de', 'clavícula', 'con', 'mínimo', 'esfuerzo', 'físico.', 'No', 'había', 'presentado', 'cólicos', 'nefríticos', 'ni', 'episodios', 'de', 'pancreatitis.', 'Ánimo', 'triste,', 'astenia', 'y', 'nicturia.', 'No', 'clínica', 'de', 'disfagia', 'ni', 'disfonía.En', 'analítica', 'realizada', 'durante', 'el', 'ingreso', 'presentaba:', 'Calcio', '13.9', 'mg/dl', '(', '8.8-10,2),', 'Fósforo', '1,8', '(2,7-4,5),', 'Fosfatasa', 'alcalina', '475', 'Ui/l', '(', '40-129),Creatinina', '1,3', 'mg/dl', '(', '0,1-1,4),PTH', '778,78', 'pg/ml(15-65),', '25-OH', 'Vitamina', 'D', '14', 'ng/ml', '(20-100).', 'Proteinograma', 'y', 'marcadores', 'tumorales', 'negativos.', 'Velocidad', 'de', 'Sedimentación', 'Glomerular', 'elevada', '(40).En', 'la', 'serie', 'radiológica', 'se', 'observaba', 'una', 'desmineralización', 'generalizada', 'junto', 'a', 'imágenes', 'de', 'resorción', 'subperióstica', 'en', 'las', 'falanges', 'de', 'las', 'manos,', 'lesiones', 'líticas', 'en', 'cabeza', 'y', 'diáfisis', 'de', 'metacarpianos', 'y', 'quistes', 'óseos', 'en', 'las', 'tibias.En', 'la', 'ecografía', 'de', 'tiroides', 'presentaba', 'un', 'tiroides', 'con', 'tamaño', 'y', 'ecoestructura', 'normales.', 'Junto', 'al', 'polo', 'inferior', 'del', 'lóbulo', 'tiroideo', 'derecho', 'aparecía', 'una', 'lesión', 'nodular,', 'bien', 'delimitada,', 'homogénea,', 'con', 'señal', 'de', 'flujo', 'en', 'su', 'interior', 'y', 'tamaño', 'aproximado', 'de', '22x17mm,', 'que', 'podía', 'ser', 'compatible', 'con', 'una', 'tumoración', 'paratiroidea', '(probablemente', 'adenoma).', 'No', 'se', 'observaban', 'adenopatías', 'cervicales', 'con', 'tamaño', 'significativo.', 'En', 'el', 'TAC', 'torácico', 'se', 'observaba', 'una', 'fractura', 'acabalgada', 'con', 'cayo', 'en', 'formación', 'en', 'el', 'tercio', 'medio', 'de', 'la', 'clavícula', 'izquierda.', 'Múltiples', 'lesiones', 'osteolíticas', 'en', 'clavícula', 'derecha,', 'ambas', 'escápulas,', 'esternón,', 'cuerpos', 'vertebrales', 'de', 'D1,', 'D4,', 'D8,', 'D9,', 'D10,', 'D11', 'y', 'L1,', 'y', 'arcos', 'costales', 'posteriores', '1o', 'bilateral,', '5o', 'derecho,', '10o', 'izquierdo,', 'con', 'destrucción', 'ósea,', 'que', 'asocia', 'masa', 'de', 'partes', 'blandas', 'paravertebral', 'anterior', 'izquierda', 'en', 'D8,', 'D9', 'y', 'D10.', 'Granulomas', 'calcificados', 'subpleurales', 'basal', 'apical', 'izquierdo', 'y', 'en', 'segmento', 'lateral', 'del', 'LM.', 'Nódulo', 'de', '5,2', 'mm', 'subpleural', 'en', 'contacto', 'con', 'el', 'mediastino', 'en', 'segmento', 'lingular', 'superior.', 'Nódulo', 'de', '3', 'mm', 'subpleural', 'basal', 'apical', 'izquierdo.', 'Nódulo', 'de', '6,6', 'mm', 'basal', 'anterior', 'izquierdo.', 'Ganglios', 'axilares,', 'hiliares', 'y', 'mediastínicos', 'de', 'tamaño', 'no', 'significativo.', 'En', 'los', 'cortes', 'de', 'hemiabdomen', 'superior,', 'no', 'se', 'observaban', 'alteraciones', 'significativas', 'en', 'las', 'porciones', 'incluidas', 'de', 'hígado,', 'vesícula,', 'bazo,', 'páncreas,', 'ni', 'riñones.', 'Suprarrenales', 'con', 'lesiones', 'nodulares', 'hipodensas', 'de', 'unos', '15', 'mm.', 'Conclusión:', 'Probable', 'origen', 'mestastásico', 'de', 'todas', 'las', 'lesiones', 'descritas.', 'En', 'la', 'gammagrafía', 'de', 'paratiroides', 'presentaba', 'imagen', 'compatible', 'con', 'adenoma', 'paratiroides', 'inferior', 'derecha', 'y', 'en', 'la', 'gammagrafía', 'ósea', 'tenía', 'múltiples', 'focos', 'hipercaptantes', 'de', 'notable', 'intensidad', 'en', 'cráneo,', 'columna', 'vertebral,', 'ambas', 'parrillas', 'costales,', 'hombros,', 'húmeros,', 'cúbitos', 'y', 'radios,', 'carpos', 'y', 'articulaciones', 'sacroilíacas,', 'ambos', 'fémures,', 'tibias', 'y', 'tarsos.', 'Primera', 'opción', 'osteítis', 'fibrosa', 'quística.Se', 'hizo', 'interconsulta', 'a', 'nuestro', 'Servicio', 'y', 'se', 'decidió', 'pautar', 'una', 'dosis', 'intravenosa', 'de', 'ácido', 'zoledrónico', '4', 'mg', 'y', 'programar', 'cirugía', 'preferente.', 'Se', 'realizó', 'una', 'hemitiroidectomia', 'derecha', 'tras', 'hallar', 'una', 'tumoración', 'de', 'consistencia', 'pétrea', 'íntimamente', 'adherida', 'a', 'hemitiroides', 'derecho.', 'El', 'diagnostico', 'anatomopatológico', 'fue', 'de', 'carcinoma', 'de', 'paratiroides', 'de', '2', 'cm', 'de', 'límites', 'no', 'definidos.', 'Descripción', 'microscopica:', 'No', 'se', 'observan', 'focos', 'de', 'necrosis', 'ni', 'pleomorfismo', 'nuclear.', 'Periféricamente', 'los', 'limites', 'no', 'están', 'definidos', 'con', 'nódulos', 'adheridos', 'al', 'tejido', 'tiroideo', 'y', 'tejido', 'fibroadiposo.', 'Periféricamente', 'no', 'se', 'evidencia', 'permeación', 'linfática,', 'ni', 'vascular', 'ni', 'perineural', 'Para', 'completar', 'el', 'estudio', 'de', 'extensión', 'se', 'solicito', 'PET-TAC', 'con', '18', 'FDG', 'que', 'mostró', 'incremento', 'metabólico', 'en', 'las', 'múltiples', 'lesiones', 'óseas', 'descritas', 'sugestivo', 'de', 'malignidad.', 'Por', 'lo', 'que', 'finalmente,', 'se', 'programó', 'biopsia', 'ósea', 'a', 'nivel', 'vertebral', 'que', 'confirma', 'la', 'osteítis', 'fibrosa', 'quística', 'sin', 'evidencia', 'de', 'infiltración', 'neoplásica.', 'Tras', 'la', 'cirugía,', 'el', 'paciente', 'mantuvo', 'niveles', 'elevados', 'de', 'PTH', 'y', 'fosfatasa', 'alcalina', 'con', 'hipocalcemia', 'leve', 'que', 'preciso', 'de', 'suplementos', 'de', 'calcio', 'vía', 'oral', 'y', 'calcifediol.', 'En', 'los', 'meses', 'posteriores', 'los', 'parámetros', 'se', 'fueron', 'normalizando', 'así', 'como', 'la', 'clínica', 'del', 'paciente', 'fue', 'remitiendo.', 'Actualmente', 'el', 'paciente', 'se', 'encuentra', 'asintomático,', 'sin', 'evidencia', 'analítica', 'de', 'recidiva', 'y', 'está', 'pendiente', 'del', 'estudio', 'genético', 'del', 'HRPT2,', 'mantenemos', 'suplementos', 'con', '600', 'mg', 'de', 'carbonato', 'cálcico/día', 'y', 'calcifediol', 'mensual', 'ante', 'la', 'hipocalciuria', 'persistente', 'que', 'presenta.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O\\n']\n",
      "699 699\n"
     ]
    }
   ],
   "source": [
    "generate_conll(sentences_file[130], filenames[130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 360\n",
      "455 455\n",
      "412 412\n",
      "555 555\n",
      "407 407\n",
      "435 435\n",
      "380 380\n",
      "916 916\n",
      "386 386\n",
      "790 790\n",
      "502 502\n",
      "479 479\n",
      "699 699\n",
      "632 632\n",
      "342 342\n",
      "350 350\n",
      "508 508\n",
      "572 572\n",
      "419 419\n",
      "576 576\n",
      "480 480\n",
      "346 346\n",
      "465 465\n",
      "26 26\n",
      "18 18\n",
      "330 330\n",
      "434 434\n",
      "681 681\n",
      "358 358\n",
      "499 499\n",
      "566 566\n",
      "448 448\n",
      "597 597\n",
      "563 563\n",
      "643 643\n",
      "453 453\n",
      "555 555\n",
      "260 260\n",
      "756 756\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for sentences_f, file_n in zip(sentences_file,filenames):\n",
    "    generate_conll(sentences_f, file_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_class = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(bert_class, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "for index, sent in enumerate(sentences_file):\n",
    "    if sent.split('/')[-1] == \"caso_clinico_endocrinologia47.txt\":\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-65-164aeb14650b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-65-164aeb14650b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tags = \"O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O    O\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
