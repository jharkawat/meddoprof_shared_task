{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_dataset):\n",
    "    \"\"\"Load dataset into memory from text file\"\"\"\n",
    "    dataset = []\n",
    "    with open(path_dataset) as f:\n",
    "        words, tags = [], []\n",
    "        # Each line of the file corresponds to one word and tag\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                line = line.strip('\\n')\n",
    "                if len(line.split()) > 0:\n",
    "                    word = line.split()[0]\n",
    "                    #tag = line.split()[-1]\n",
    "                else:\n",
    "                    word = line.split()[0]\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                try:\n",
    "                    if len(word) > 0:\n",
    "                        word = str(word)\n",
    "                        words.append(word)\n",
    "                        #tags.append(tag)\n",
    "                except Exception as e:\n",
    "                    print('An exception was raised, skipping a word: {}'.format(e))\n",
    "            else:\n",
    "                if len(words) > 0:\n",
    "                    #assert len(words) == len(tags)\n",
    "                    dataset.append((words))\n",
    "                    words, tags = [], []\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, save_dir,file_name):\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    print('Saving in {}...'.format(save_dir))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Export the dataset\n",
    "    with open(os.path.join(save_dir, file_name), 'w') as file_sentences:\n",
    "        for sent in dataset:\n",
    "            for word in sent:\n",
    "                file_sentences.writelines('%s\\n' %word)\n",
    "            file_sentences.writelines('\\n')\n",
    "            #file_tags.write('{}\\n'.format(' '.join(tags)))\n",
    "        file_sentences.writelines('\\n')\n",
    "        file_sentences.writelines('\\n')\n",
    "    print('- done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'submission_Conll'\n",
    "import glob\n",
    "filenames = glob.glob('output_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_files = []\n",
    "for filename in filenames:\n",
    "    pred_tags_file = load_dataset(filename)\n",
    "    pred_tags_files.append(pred_tags_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/meddo/test_data/Conll_Format/casos_clinicos_profesiones230.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-df1801163f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpred_tags_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mre_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwords_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mre_name\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-97ca474ef1bf>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path_dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Load dataset into memory from text file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Each line of the file corresponds to one word and tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/meddo/test_data/Conll_Format/casos_clinicos_profesiones230.txt'"
     ]
    }
   ],
   "source": [
    "root_dir = 'data/meddo/test_data/Conll_Format/'\n",
    "for filename in filenames:\n",
    "    pred_tags_file = load_dataset(filename)\n",
    "    re_name = filename.split('/')[-1]\n",
    "    words_file = load_dataset(root_dir + re_name )\n",
    "    for i in range(len(pred_tags_file)):\n",
    "        for j in range(len(pred_tags_file[i])):\n",
    "            words_file[i][j] = words_file[i][j]+\"\\t\"+pred_tags_file[i][j]\n",
    "    save_dataset(words_file,save_dir,re_name)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_data/casos_clinicos_profesiones230.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file = glob.glob(\"./data/meddo/interactive/sentences/*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_file[0].split(\"/\")[-1] == filenames[0].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el SPRL del Hospital Clínico de Valencia se atiende a una trabajadora de 30 años residente de primer año de una especialidad médica, que acude para un reconocimiento inicial obligatorio. Durante la anamnesis y la exploración física no se encuentran alteraciones importantes. Se le solicita analítica y serología objetivándose en los resultados una importante elevación de transaminasas. Se cita a la trabajadora en el SPRL para comunicar los resultados y poder filiar el origen. Se le pasa el cuestionario CAGE camuflado resultando positivo para detección de consumo de alcohol, que es admitido por la trabajadora en ese momento.\n",
      "\n",
      "Se le explica a la trabajadora las posibles repercusiones tanto a nivel laboral como personal, se le ofrece la posibilidad de derivar al programa PAIME con el consentimiento de la trabajadora. En el mismo se le asigna un psiquiatra, se inicia tratamiento de deshabituación y se le deriva a ingreso para desintoxicación.\n",
      "\n",
      "Tras el alta hospitalaria, el médico del trabajo y psiquiatra de PAIME conjuntamente deciden realizar supervisión y seguimiento del tratamiento farmacológico en el SPRL de forma que la trabajadora se reincorpora a sus tareas habituales.\n",
      "\n",
      "Tras un rotatorio externo deja de tener contacto con SPRL y unos meses después informa al SPRL de recaída. Se contacta con psiquiatra de PAIME que confirma la recaída y consensua nuevamente seguimiento y supervisión de tratamiento farmacológico.\n",
      "\n",
      "La trabajadora continuó su programa formativo, continuó acudiendo periódicamente al psiquiatra de PAIME y al SPRL para supervisar la toma de la medicación aversiva.\n"
     ]
    }
   ],
   "source": [
    "with open(sentences_file[3]) as f, open(filenames[3]) as q:\n",
    "    assert sentences_file[3].split(\"/\")[-1] == filenames[3].split(\"/\")[-1]\n",
    "    words, tags, comb_str = [], [], []\n",
    "    for line_sentence, line_tags in zip(f,q):\n",
    "        print(line_sentence)\n",
    "        assert len(line_sentence.split()) == len(line_tags.split('    '))\n",
    "        words.append(line_sentence.split())\n",
    "        tags.append(line_tags.split('    '))\n",
    "    for tokens,tag in zip(words,tags):\n",
    "        for token,t in zip(tokens, tag):\n",
    "            comb_str.append(token+ \"\\t\" + t)\n",
    "    \n",
    "    with open(\"./temp/\"+sentences_file[3].split(\"/\")[-1], 'w') as r:\n",
    "        for item in comb_str:\n",
    "            r.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_class = 'mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(bert_class, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34masset\u001b[0m/                 evalution2.py   \u001b[01;34m__pycache__\u001b[0m/\r\n",
      "build_dataset_tags.py  \u001b[01;34mexperiments\u001b[0m/    Readme.md\r\n",
      "\u001b[01;34mdata\u001b[0m/                  interactive.py  SequenceTagger.py\r\n",
      "data_loader.py         LICENSE         submit_format_generator.ipynb\r\n",
      "\u001b[01;34mdataset\u001b[0m/               metrics.py      train.py\r\n",
      "evaluate.py            \u001b[01;34moutput_data\u001b[0m/    utils.py\r\n"
     ]
    }
   ],
   "source": [
    "with open(path_dataset) as f:\n",
    "    words, tags = [], []\n",
    "        # Each line of the file corresponds to one word and tag\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "                line = line.strip('\\n')\n",
    "                if len(line.split()) > 0:\n",
    "                    word = line.split()[0]\n",
    "                    #tag = line.split()[-1]\n",
    "                else:\n",
    "                    word = line.split()[0]\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                try:\n",
    "                    if len(word) > 0:\n",
    "                        word = str(word)\n",
    "                        words.append(word)\n",
    "                        #tags.append(tag)\n",
    "                except Exception as e:\n",
    "                    print('An exception was raised, skipping a word: {}'.format(e))\n",
    "            else:\n",
    "                if len(words) > 0:\n",
    "                    #assert len(words) == len(tags)\n",
    "                    dataset.append((words))\n",
    "                    words, tags = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
